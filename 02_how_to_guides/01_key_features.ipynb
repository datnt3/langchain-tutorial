{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e623afd",
   "metadata": {},
   "source": [
    "# 1. How to return structured data from a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d301de",
   "metadata": {},
   "source": [
    "## The `.with_structured_output()` method\n",
    "- Takes a schema as input specifying names, types, and description of the desired output attributes.\n",
    "- The method returns a model-like Runnable, which outputs objects corresponding to the given schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd762f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46fa94",
   "metadata": {},
   "source": [
    "#### Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2041774c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "  \"\"\"Joke to tell user.\"\"\"\n",
    "  setup: str = Field(description=\"The setup of the joke\")\n",
    "  punchline: str = Field(description=\"The punchline to the joke\")\n",
    "  rating: Optional[str] = Field(\n",
    "    default=None,\n",
    "    description=\"How funny the joke is, from 1 to 10\"\n",
    "  )\n",
    "  \n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3c11b",
   "metadata": {},
   "source": [
    "#### TypedDict or JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b860418d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why was the cat sitting on the computer?',\n",
       " 'rating': 8.0,\n",
       " 'punchline': 'To keep an eye on the mouse!'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class Joke(TypedDict):\n",
    "  \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "  setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "  punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "  rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9f2a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the cat cross the road?',\n",
       " 'punchline': 'Because they are feline good!'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "            \"default\": None,\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d92bd9",
   "metadata": {},
   "source": [
    "### Choosing between multiple schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c0a11",
   "metadata": {},
   "source": [
    "#### Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fb66fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown field for Schema: anyOf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/rules/message.py:36\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Try the fast path first.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message Schema has no \"anyOf\" field.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/rules/message.py:36\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Try the fast path first.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message Schema has no \"anyOf\" field.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFinalResponse\u001b[39;00m(BaseModel):\n\u001b[1;32m     21\u001b[0m     final_output: Union[Joke, ConversationalResponse]\n\u001b[0;32m---> 24\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFinalResponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m structured_llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke about cats\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:1925\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.with_structured_output\u001b[0;34m(self, schema, method, include_raw, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m   1917\u001b[0m         response_mime_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1918\u001b[0m         response_schema\u001b[38;5;241m=\u001b[39mschema_json,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         },\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1925\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tool_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_basemodel_subclass_safe(schema):\n\u001b[1;32m   1927\u001b[0m         parser \u001b[38;5;241m=\u001b[39m PydanticToolsParser(tools\u001b[38;5;241m=\u001b[39m[schema], first_tool_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:2016\u001b[0m, in \u001b[0;36m_get_tool_name\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_openai_tool(cast(Dict, tool))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:2010\u001b[0m, in \u001b[0;36m_get_tool_name\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_tool_name\u001b[39m(\n\u001b[1;32m   2007\u001b[0m     tool: Union[_ToolDict, GoogleTool, Dict],\n\u001b[1;32m   2008\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2010\u001b[0m         genai_tool \u001b[38;5;241m=\u001b[39m tool_to_dict(\u001b[43mconvert_to_genai_function_declarations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2011\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m genai_tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_declarations\u001b[39m\u001b[38;5;124m\"\u001b[39m]][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# other TypedDict\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/_function_utils.py:201\u001b[0m, in \u001b[0;36mconvert_to_genai_function_declarations\u001b[0;34m(tools)\u001b[0m\n\u001b[1;32m    199\u001b[0m             gapic_tool\u001b[38;5;241m.\u001b[39mcode_execution \u001b[38;5;241m=\u001b[39m gapic\u001b[38;5;241m.\u001b[39mCodeExecution(tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         fd \u001b[38;5;241m=\u001b[39m \u001b[43m_format_to_gapic_function_declaration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         gapic_tool\u001b[38;5;241m.\u001b[39mfunction_declarations\u001b[38;5;241m.\u001b[39mappend(fd)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gapic_tool\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/_function_utils.py:225\u001b[0m, in \u001b[0;36m_format_to_gapic_function_declaration\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _format_base_tool_to_function_declaration(tool)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_basemodel_subclass_safe(tool):\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_pydantic_to_genai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(k \u001b[38;5;129;01min\u001b[39;00m tool \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_google_genai/_function_utils.py:301\u001b[0m, in \u001b[0;36m_convert_pydantic_to_genai_function\u001b[0;34m(pydantic_model, tool_name, tool_description)\u001b[0m\n\u001b[1;32m    299\u001b[0m schema \u001b[38;5;241m=\u001b[39m dereference_refs(schema)\n\u001b[1;32m    300\u001b[0m schema\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 301\u001b[0m function_declaration \u001b[38;5;241m=\u001b[39m \u001b[43mgapic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunctionDeclaration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_description\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool_description\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_properties_from_schema_any\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: use _dict_to_gapic_schema() if possible\u001b[39;49;00m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"items\": _get_items_from_schema_any(\u001b[39;49;00m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     schema\u001b[39;49;00m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ),  # TODO: fix it https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling?hl#schema\u001b[39;49;00m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTYPE_ENUM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_declaration\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/message.py:728\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key)\n\u001b[1;32m    726\u001b[0m     )\n\u001b[0;32m--> 728\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     params[key] \u001b[38;5;241m=\u001b[39m pb_value\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/marshal.py:235\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    232\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 235\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/rules/message.py:46\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_descriptor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalue)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_pb\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/message.py:728\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key)\n\u001b[1;32m    726\u001b[0m     )\n\u001b[0;32m--> 728\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     params[key] \u001b[38;5;241m=\u001b[39m pb_value\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/marshal.py:233\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    229\u001b[0m     proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mhas_options\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mGetOptions()\u001b[38;5;241m.\u001b[39mmap_entry\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    235\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/marshal.py:233\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    229\u001b[0m     proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mhas_options\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mGetOptions()\u001b[38;5;241m.\u001b[39mmap_entry\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecursive_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    235\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/marshal.py:235\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    232\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 235\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/marshal/rules/message.py:46\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_descriptor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalue)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# If we have a TypeError, ValueError or AttributeError,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# - a missing key issue due to nested struct. See: https://github.com/googleapis/proto-plus-python/issues/424.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# - a missing key issue due to nested duration. See: https://github.com/googleapis/google-cloud-python/issues/13350.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_pb\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/langchain/lib/python3.10/site-packages/proto/message.py:724\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_unknown_fields:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key)\n\u001b[1;32m    726\u001b[0m     )\n\u001b[1;32m    728\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m marshal\u001b[38;5;241m.\u001b[39mto_proto(pb_type, value)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown field for Schema: anyOf"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    final_output: Union[Joke, ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(FinalResponse)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7f231",
   "metadata": {},
   "source": [
    "#### TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65524051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_output': {'response': 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class Joke(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "class ConversationalResponse(TypedDict):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n",
    "\n",
    "\n",
    "class FinalResponse(TypedDict):\n",
    "    final_output: Union[Joke, ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(FinalResponse)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c5fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_output': {'response': \"I'm doing well, thank you for asking! How can I help you today?\"}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d43a4d",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': 'Why did the cat join the orchestra?', 'rating': 8.0, 'punchline': \"Because they like to play with their 'mewsic'!\"}\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class Joke(TypedDict):\n",
    "  \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "  setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "  punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "  rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
    "  print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc177c8",
   "metadata": {},
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a148ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n",
    "\n",
    "Here are some examples of jokes:\n",
    "\n",
    "example_user: Tell me a joke about planes\n",
    "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
    "\n",
    "example_user: Tell me another joke about planes\n",
    "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
    "\n",
    "example_user: Now about caterpillars\n",
    "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
    "few_shot_structured_llm = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Timber',\n",
       " 'rating': 7.0,\n",
       " 'punchline': \"Timber! Better open up, or I'll peck my way in!\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Croc',\n",
       " 'rating': 7.0,\n",
       " 'punchline': \"Croc-o-dile, I'm going to snap!\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "examples = [\n",
    "  HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),\n",
    "  AIMessage(\n",
    "    \"\",\n",
    "    name=\"example_assistant\",\n",
    "    tool_calls=[\n",
    "      {\n",
    "        \"name\": \"joke\",\n",
    "        \"args\": {\n",
    "          \"setup\": \"Why don't planes ever get tired?\",\n",
    "          \"punchline\": \"Because they have rest wings!\",\n",
    "          \"rating\": 2\n",
    "        },\n",
    "        \"id\": \"1\"\n",
    "      }\n",
    "    ]\n",
    "  ),\n",
    "  ToolMessage(\"\", tool_call_id=\"1\"),\n",
    "  HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),\n",
    "  AIMessage(\n",
    "      \"\",\n",
    "      name=\"example_assistant\",\n",
    "      tool_calls=[\n",
    "          {\n",
    "              \"name\": \"joke\",\n",
    "              \"args\": {\n",
    "                  \"setup\": \"Cargo\",\n",
    "                  \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",\n",
    "                  \"rating\": 10,\n",
    "              },\n",
    "              \"id\": \"2\",\n",
    "          }\n",
    "      ],\n",
    "  ),\n",
    "  ToolMessage(\"\", tool_call_id=\"2\"),\n",
    "  HumanMessage(\"Now about caterpillars\", name=\"example_user\"),\n",
    "  AIMessage(\n",
    "      \"\",\n",
    "      name=\"example_assistant\",\n",
    "      tool_calls=[\n",
    "          {\n",
    "              \"name\": \"joke\",\n",
    "              \"args\": {\n",
    "                  \"setup\": \"Caterpillar\",\n",
    "                  \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",\n",
    "                  \"rating\": 5,\n",
    "              },\n",
    "              \"id\": \"3\",\n",
    "          }\n",
    "      ],\n",
    "  ),\n",
    "  ToolMessage(\"\", tool_call_id=\"3\"),\n",
    "]\n",
    "\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") \\\n",
    "and the final punchline (the response to \"<setup> who?\").\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke({\n",
    "  \"examples\": examples,\n",
    "  \"input\": \"crocodiles\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729fcc6",
   "metadata": {},
   "source": [
    "### (Advanced) Specifying the method for structuring outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0758d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why are cats so bad at poker?', punchline=\"Because they're always feline good!\", rating=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "  \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830fb7b",
   "metadata": {},
   "source": [
    "### (Advanced) Raw outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c5f6d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': AIMessage(content='', additional_kwargs={'function_call': {'name': 'Joke', 'arguments': '{\"setup\": \"Why are cats bad at poker?\", \"rating\": 8.0, \"punchline\": \"Because they like to play with their mouseterpiece!\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--336dc7c4-a19a-4984-9bce-03c3747652f7-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why are cats bad at poker?', 'rating': 8.0, 'punchline': 'Because they like to play with their mouseterpiece!'}, 'id': '3a1b1a74-86e8-4c92-b4ae-1ddbf7867ae3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 96, 'total_tokens': 187, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}),\n",
       " 'parsed': Joke(setup='Why are cats bad at poker?', punchline='Because they like to play with their mouseterpiece!', rating=8),\n",
       " 'parsing_error': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(Joke, include_raw=True)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6ae9f",
   "metadata": {},
   "source": [
    "## Prompting and parsing model outputs directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd6383",
   "metadata": {},
   "source": [
    "### Using `PydanicOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce738288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "  \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "  name: str = Field(..., description=\"The name of a person\")\n",
    "  height_in_meters: float = Field(\n",
    "    ...,\n",
    "    description=\"The height of a person in meters.\"\n",
    "  )\n",
    "  \n",
    "class People(BaseModel):\n",
    "  \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "  people: List[Person]\n",
    "\n",
    "# Setup a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"Answer the user query. wrap the output in `json` tags\\n{format_instructions}\"\n",
    "    ),\n",
    "    (\n",
    "      \"human\",\n",
    "      \"{query}\"\n",
    "    )\n",
    "  ]\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d0cf2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of a person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of a person in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
      "```\n",
      "Human: Anna is 23 years old and she is 6 ft tall\n"
     ]
    }
   ],
   "source": [
    "query = \"Anna is 23 years old and she is 6 ft tall\"\n",
    "\n",
    "print(prompt.invoke({\"query\": query}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6568b224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2be8",
   "metadata": {},
   "source": [
    "### Custom Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "410a5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "    \n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Output your answer as JSON that  \"\n",
    "            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"\n",
    "            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(schema=People.model_json_schema()) \n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = message.content\n",
    "    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f1394dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json\n",
      "{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}\n",
      "\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.format_prompt(query=query).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be3312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4814acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | extract_json\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e432875",
   "metadata": {},
   "source": [
    "## Combining with Addtional Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a18d4",
   "metadata": {},
   "source": [
    "# 2. How to use a model to call tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbe9bb",
   "metadata": {},
   "source": [
    "## Defining tool schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e9d3c",
   "metadata": {},
   "source": [
    "### Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab99600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a2a58",
   "metadata": {},
   "source": [
    "### LangChain Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae596da5",
   "metadata": {},
   "source": [
    "### Pydantic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8aca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366415d",
   "metadata": {},
   "source": [
    "### TypedDict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e721c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class add(TypedDict):\n",
    "  \"\"\"Add two integers.\"\"\"\n",
    "  a: Annotated[int, ..., \"First integer\"]\n",
    "  b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "class multiply(TypedDict):\n",
    "  \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "  a: Annotated[int, ..., \"First integer\"]\n",
    "  b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "tools = [add, multiply]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202496af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab74d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"b\": 12.0, \"a\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--27641626-0a7d-4ffe-9f7f-aaf9691fd495-0', tool_calls=[{'name': 'multiply', 'args': {'b': 12.0, 'a': 3.0}, 'id': '2de847c3-8734-4a2c-b44d-330a7152625d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 104, 'output_tokens': 103, 'total_tokens': 207, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 84}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aaaf9a",
   "metadata": {},
   "source": [
    "## Tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1b2193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'b': 49.0, 'a': 11.0},\n",
       "  'id': '9c892ad0-b1ec-49c6-9fdd-4bc473e2b10e',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca31682",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff621d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[add(a=11, b=49)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticToolsParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class add(BaseModel):\n",
    "  \"\"\"Add two integers\"\"\"\n",
    "  a: int = Field(..., description=\"First integer\")\n",
    "  b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "class multiply(BaseModel):\n",
    "  \"\"\"Multiply two integers\"\"\"\n",
    "  a: int = Field(..., description=\"First integer\")\n",
    "  b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865e053",
   "metadata": {},
   "source": [
    "# 3. How to stream runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422932ac",
   "metadata": {},
   "source": [
    "## Using Stream\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa8448",
   "metadata": {},
   "source": [
    "### LLMs and Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef390938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba88741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is **most commonly blue**.\n",
      "\n",
      "This is due to a phenomenon called **Rayleigh scattering**, where shorter wavelengths of light (like blue and violet) are scattered more efficiently by the molecules in Earth's atmosphere than longer wavelengths (like| red and yellow). Because blue light is scattered in all directions, it makes the sky appear blue to our eyes.\n",
      "\n",
      "However, the sky can be many other colors depending on various factors:\n",
      "\n",
      "*   **Red, orange, pink, or| purple:** During **sunrise and sunset**, when the sun's light has to travel through more of the atmosphere. Most of the blue light is scattered away, leaving the longer red, orange, and yellow wavelengths to reach our eyes.\n",
      "*|   **Grey or white:** On **cloudy or overcast days**, as clouds are made of water droplets or ice crystals that scatter all wavelengths of light equally, making them appear white or grey depending on their thickness and the amount of light they block|.\n",
      "*   **Dark grey or even black:** During **severe thunderstorms** or very heavy rain, when dense clouds block almost all light.\n",
      "*   **Yellow or brownish:** In areas with significant **air pollution** (smog,| dust, etc.) which can scatter light in different ways.\n",
      "*   **Black:** At **night**, when there's no direct sunlight to scatter, allowing us to see the stars and the dark expanse of space.\n",
      "\n",
      "So, while we| typically think of the sky as blue, its color is constantly changing based on the time of day, weather conditions, and atmospheric composition!|"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "  chunks.append(chunk)\n",
    "  print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f455dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common color of the sky during the day is **blue**.\n",
      "\n",
      "This is due to a phenomenon called **Rayleigh scattering**. Here's a simple explanation:\n",
      "\n",
      "*   Sunlight is made up of all the colors of the rainbow|.\n",
      "*   Earth's atmosphere is made of tiny molecules of nitrogen and oxygen.\n",
      "*   Blue light, which has shorter wavelengths, is scattered more efficiently by these tiny molecules than longer wavelengths like red or yellow.\n",
      "*   |When sunlight enters our atmosphere, the blue light is scattered in all directions, making the sky appear blue to our eyes.\n",
      "\n",
      "However, the sky can also be many other colors depending on the time of day, weather, and atmospheric conditions:|\n",
      "\n",
      "*   **Orange, Red, Pink, or Purple** at sunrise or sunset: When the sun is low on the horizon, its light has to travel through more of the atmosphere. Most of the blue light has already been scattered away|, leaving the longer wavelengths (red, orange, yellow) to reach our eyes.\n",
      "*   **Gray or White** when cloudy or overcast: Clouds are made of water droplets or ice crystals that scatter all colors of light equally, making| them appear white or, if they're very thick and block a lot of light, gray.\n",
      "*   **Black** at night: At night, with no direct sunlight illuminating the atmosphere, the sky appears black, allowing us to| see stars and other celestial objects.\n",
      "\n",
      "So, while blue is the most common answer during the day, the sky's color is constantly changing!|"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
    "  chunks.append(chunk)\n",
    "  print(chunk.content, end=\"|\", flush=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff20f808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"The most common color of the sky during the day is **blue**.\\n\\nThis is due to a phenomenon called **Rayleigh scattering**. Here's a simple explanation:\\n\\n*   Sunlight is made up of all the colors of the rainbow\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--4ec63ec7-4941-491f-afdf-f86e8ff13033', usage_metadata={'input_tokens': 7, 'output_tokens': 900, 'total_tokens': 907, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 851}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
